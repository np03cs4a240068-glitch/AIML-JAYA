{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rktAgFvf8FJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Read the dataset\n",
        "df = pd.read_csv('student.csv')\n",
        "print(\"Dataset loaded successfully\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SEoF3st0g5HZ",
        "outputId": "8c1ee7cb-6681-489b-b2ad-0067793901d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Print top 5 and bottom 5 rows\n",
        "print(\"\\nTop 5 rows:\")\n",
        "print(df.head())\n",
        "\n",
        "print(\"\\nBottom 5 rows:\")\n",
        "print(df.tail())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QzgAXOSg87n",
        "outputId": "976ea25b-89df-4fd1-d8bc-3917f72299b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 5 rows:\n",
            "   Math  Reading  Writing\n",
            "0    48       68       63\n",
            "1    62       81       72\n",
            "2    79       80       78\n",
            "3    76       83       79\n",
            "4    59       64       62\n",
            "\n",
            "Bottom 5 rows:\n",
            "     Math  Reading  Writing\n",
            "995    72       74       70\n",
            "996    73       86       90\n",
            "997    89       87       94\n",
            "998    83       82       78\n",
            "999    66       66       72\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Print dataset information\n",
        "print(\"\\nDataset Information:\")\n",
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uesL_BcIg9yq",
        "outputId": "6863fe4b-7070-4d67-fb95-92af33344565"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dataset Information:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1000 entries, 0 to 999\n",
            "Data columns (total 3 columns):\n",
            " #   Column   Non-Null Count  Dtype\n",
            "---  ------   --------------  -----\n",
            " 0   Math     1000 non-null   int64\n",
            " 1   Reading  1000 non-null   int64\n",
            " 2   Writing  1000 non-null   int64\n",
            "dtypes: int64(3)\n",
            "memory usage: 23.6 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Descriptive statistics\n",
        "print(\"\\nDescriptive Statistics:\")\n",
        "print(df.describe())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-awjZDKg_h4",
        "outputId": "1ffbfdff-8ec7-449c-ad6a-32209cd2d212"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Descriptive Statistics:\n",
            "              Math      Reading      Writing\n",
            "count  1000.000000  1000.000000  1000.000000\n",
            "mean     67.290000    69.872000    68.616000\n",
            "std      15.085008    14.657027    15.241287\n",
            "min      13.000000    19.000000    14.000000\n",
            "25%      58.000000    60.750000    58.000000\n",
            "50%      68.000000    70.000000    69.500000\n",
            "75%      78.000000    81.000000    79.000000\n",
            "max     100.000000   100.000000   100.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Split data into Feature (X) and Label (Y)\n",
        "# First check the actual column names\n",
        "print(\"\\nColumn names in dataset:\")\n",
        "print(df.columns.tolist())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3Obx08hhBZ4",
        "outputId": "1cb70486-b90e-4264-d858-cf0813f3d57a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Column names in dataset:\n",
            "['Math', 'Reading', 'Writing']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the correct column names from the dataset\n",
        "X = df.iloc[:, [0, 1]]  # First two columns (math and reading)\n",
        "y = df.iloc[:, 2]  # Third column (writing)\n",
        "\n",
        "print(\"\\nFeature matrix (X) shape:\", X.shape)\n",
        "print(\"Label vector (y) shape:\", y.shape)\n",
        "\n",
        "print(\"\\nFeatures (X):\")\n",
        "print(X.head())\n",
        "\n",
        "print(\"\\nLabel (y):\")\n",
        "print(y.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2bPO73yhDQ_",
        "outputId": "8b3a689c-169d-4a73-a040-7410a78f8304"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Feature matrix (X) shape: (1000, 2)\n",
            "Label vector (y) shape: (1000,)\n",
            "\n",
            "Features (X):\n",
            "   Math  Reading\n",
            "0    48       68\n",
            "1    62       81\n",
            "2    79       80\n",
            "3    76       83\n",
            "4    59       64\n",
            "\n",
            "Label (y):\n",
            "0    63\n",
            "1    72\n",
            "2    78\n",
            "3    79\n",
            "4    62\n",
            "Name: Writing, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To-Do 2: Create matrices for linear regression (Y = W^T X)\n",
        "\n",
        "# Assumption: No bias or intercept\n",
        "\n",
        "print(\"Assumption: No bias or intercept term in the model\")\n",
        "print(\"Model equation: Y = W^T X\")\n",
        "\n",
        "# Create the matrices according to Y = W^T X\n",
        "\n",
        "# X matrix: shape (d x n) where d = number of features, n = number of samples\n",
        "X_matrix = X.T.values  # Transpose to get (d x n)\n",
        "\n",
        "# Y matrix: shape (n x 1) where n = number of samples\n",
        "Y_matrix = y.values.reshape(-1, 1)  # Reshape to column vector\n",
        "\n",
        "# W matrix: shape (d x 1) where d = number of features\n",
        "d = X_matrix.shape[0]  # number of features\n",
        "W_matrix = np.zeros((d, 1))  # Weight vector initialized with zeros\n",
        "\n",
        "print(f\"\\nMatrix dimensions:\")\n",
        "print(f\"X matrix: {X_matrix.shape} (d x n)\")\n",
        "print(f\"W matrix: {W_matrix.shape} (d x 1)\")\n",
        "print(f\"Y matrix: {Y_matrix.shape} (n x 1)\")\n",
        "\n",
        "print(f\"\\nwhere d = {d} features, n = {X_matrix.shape[1]} samples\")\n",
        "\n",
        "print(\"\\nCreated matrices:\")\n",
        "\n",
        "print(\"\\nW matrix (weight vector):\")\n",
        "print(\"W =\")\n",
        "print(W_matrix)\n",
        "print(f\"Shape: {W_matrix.shape}, W ∈ R^d where d = {d}\")\n",
        "\n",
        "print(\"\\nX matrix (feature matrix):\")\n",
        "print(\"X =\")\n",
        "print(X_matrix[:, :5])  # Show first 5 samples\n",
        "print(f\"Shape: {X_matrix.shape}, X ∈ R^(d×n) where d = {d}, n = {X_matrix.shape[1]}\")\n",
        "\n",
        "print(\"\\nY matrix (target vector):\")\n",
        "print(\"Y =\")\n",
        "print(Y_matrix[:5])  # Show first 5 values\n",
        "print(f\"Shape: {Y_matrix.shape}, Y ∈ R^n where n = {Y_matrix.shape[0]}\")\n",
        "\n",
        "print(\"\\nNote: The feature matrix X does not include a column of 1s,\")\n",
        "print(\"as it assumes the absence of a bias term in the model.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Wi4XoEzh00w",
        "outputId": "a95299ab-976e-4394-9a87-dde6a3da1539"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assumption: No bias or intercept term in the model\n",
            "Model equation: Y = W^T X\n",
            "\n",
            "Matrix dimensions:\n",
            "X matrix: (2, 1000) (d x n)\n",
            "W matrix: (2, 1) (d x 1)\n",
            "Y matrix: (1000, 1) (n x 1)\n",
            "\n",
            "where d = 2 features, n = 1000 samples\n",
            "\n",
            "Created matrices:\n",
            "\n",
            "W matrix (weight vector):\n",
            "W =\n",
            "[[0.]\n",
            " [0.]]\n",
            "Shape: (2, 1), W ∈ R^d where d = 2\n",
            "\n",
            "X matrix (feature matrix):\n",
            "X =\n",
            "[[48 62 79 76 59]\n",
            " [68 81 80 83 64]]\n",
            "Shape: (2, 1000), X ∈ R^(d×n) where d = 2, n = 1000\n",
            "\n",
            "Y matrix (target vector):\n",
            "Y =\n",
            "[[63]\n",
            " [72]\n",
            " [78]\n",
            " [79]\n",
            " [62]]\n",
            "Shape: (1000, 1), Y ∈ R^n where n = 1000\n",
            "\n",
            "Note: The feature matrix X does not include a column of 1s,\n",
            "as it assumes the absence of a bias term in the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To-Do 3: Split dataset into training and test sets\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split using 80-20 ratio (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Dataset split completed (80-20 split)\")\n",
        "print(f\"\\nTraining set:\")\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "\n",
        "print(f\"\\nTest set:\")\n",
        "print(f\"X_test shape: {X_test.shape}\")\n",
        "print(f\"y_test shape: {y_test.shape}\")\n",
        "\n",
        "print(f\"\\nTotal samples: {len(X)}\")\n",
        "print(f\"Training samples: {len(X_train)} ({len(X_train)/len(X)*100:.1f}%)\")\n",
        "print(f\"Test samples: {len(X_test)} ({len(X_test)/len(X)*100:.1f}%)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgc5t_odh81b",
        "outputId": "e617e673-0c64-47ef-8354-3f212f0f4fb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset split completed (80-20 split)\n",
            "\n",
            "Training set:\n",
            "X_train shape: (800, 2)\n",
            "y_train shape: (800,)\n",
            "\n",
            "Test set:\n",
            "X_test shape: (200, 2)\n",
            "y_test shape: (200,)\n",
            "\n",
            "Total samples: 1000\n",
            "Training samples: 800 (80.0%)\n",
            "Test samples: 200 (20.0%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To-Do 4: Building a Cost Function\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def cost_function(X, Y, W):\n",
        "    \"\"\"\n",
        "    This function finds the Mean Square Error.\n",
        "\n",
        "    Parameters:\n",
        "    X: Feature Matrix (d x n) or (n x d)\n",
        "    Y: Target Matrix (n,) or (n x 1)\n",
        "    W: Weight Matrix (d,) or (d x 1)\n",
        "\n",
        "    Output:\n",
        "    cost: accumulated mean square error\n",
        "    \"\"\"\n",
        "    # Ensure proper shapes\n",
        "    if X.ndim == 2 and X.shape[0] < X.shape[1]:\n",
        "        # X is (d x n), transpose it to (n x d)\n",
        "        X = X.T\n",
        "\n",
        "    if W.ndim == 1:\n",
        "        W = W.reshape(-1, 1)\n",
        "\n",
        "    if Y.ndim == 1:\n",
        "        Y = Y.reshape(-1, 1)\n",
        "\n",
        "    # Number of samples\n",
        "    n = X.shape[0]\n",
        "\n",
        "    # Predictions: Y_pred = X * W\n",
        "    Y_pred = np.dot(X, W)\n",
        "\n",
        "    # Mean Square Error: MSE = (1/n) * sum((Y - Y_pred)^2)\n",
        "    cost = (1 / n) * np.sum((Y - Y_pred) ** 2)\n",
        "\n",
        "    return cost\n",
        "\n",
        "print(\"Cost function defined successfully\")\n",
        "print(\"\\nFunction: cost_function(X, Y, W)\")\n",
        "print(\"Calculates Mean Square Error (MSE)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3e45iY5Si_tD",
        "outputId": "b5f092f1-1140-440d-a2eb-c74853d662ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cost function defined successfully\n",
            "\n",
            "Function: cost_function(X, Y, W)\n",
            "Calculates Mean Square Error (MSE)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To-Do 5: Testing the Cost Function\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Test case\n",
        "X_test = np.array([[1, 2], [3, 4], [5, 6]])\n",
        "Y_test = np.array([3, 7, 11])\n",
        "W_test = np.array([1, 1])\n",
        "\n",
        "cost = cost_function(X_test, Y_test, W_test)\n",
        "\n",
        "if cost == 0:\n",
        "    print(\"Proceed Further\")\n",
        "else:\n",
        "    print(\"something went wrong: Reimplement a cost function\")\n",
        "\n",
        "print(\"Cost function output:\", cost_function(X_test, Y_test, W_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-bY39m67jAqd",
        "outputId": "2b8875bc-7de5-478c-dfa3-049df04ae8a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Proceed Further\n",
            "Cost function output: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To-Do 6: Implement Gradient Descent\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def gradient_descent(X, Y, W, alpha, iterations):\n",
        "    \"\"\"\n",
        "    Perform gradient descent to optimize the parameters of a linear regression model.\n",
        "\n",
        "    Parameters:\n",
        "    X (numpy.ndarray): Feature matrix (m x n).\n",
        "    Y (numpy.ndarray): Target vector (m x 1).\n",
        "    W (numpy.ndarray): Initial guess for parameters (n x 1).\n",
        "    alpha (float): Learning rate.\n",
        "    iterations (int): Number of iterations for gradient descent.\n",
        "\n",
        "    Returns:\n",
        "    tuple: A tuple containing the final optimized parameters (W_update) and the history of cost values.\n",
        "    W_update (numpy.ndarray): Updated parameters (n x 1).\n",
        "    cost_history (list): History of cost values over iterations.\n",
        "    \"\"\"\n",
        "    # Initialize cost history\n",
        "    cost_history = [0] * iterations\n",
        "\n",
        "    # Number of samples\n",
        "    m = len(Y)\n",
        "\n",
        "    for iteration in range(iterations):\n",
        "        # Step 1: Hypothesis Values\n",
        "        Y_pred = np.dot(X, W)\n",
        "\n",
        "        # Step 2: Difference between Hypothesis and Actual Y\n",
        "        loss = Y_pred - Y\n",
        "\n",
        "        # Step 3: Gradient Calculation\n",
        "        dw = (1 / m) * np.dot(X.T, loss)\n",
        "\n",
        "        # Step 4: Updating Values of W using Gradient\n",
        "        W_update = W - alpha * dw\n",
        "\n",
        "        # Step 5: New Cost Value\n",
        "        cost = cost_function(X, Y, W_update)\n",
        "        cost_history[iteration] = cost\n",
        "\n",
        "        # Update W for next iteration\n",
        "        W = W_update\n",
        "\n",
        "    return W_update, cost_history\n",
        "\n",
        "print(\"Gradient descent function defined successfully\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVyz9uejjFu2",
        "outputId": "878f77be-8d70-408b-d86b-825198fc85aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient descent function defined successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To-Do 7: Test Gradient Descent function\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Generate random test data\n",
        "np.random.seed(0)  # For reproducibility\n",
        "X = np.random.rand(100, 3)  # 100 samples, 3 features\n",
        "Y = np.random.rand(100)\n",
        "W = np.random.rand(3)  # Initial guess for parameters\n",
        "\n",
        "# Set hyperparameters\n",
        "alpha = 0.01\n",
        "iterations = 1000\n",
        "\n",
        "# Test the gradient_descent function\n",
        "final_params, cost_history = gradient_descent(X, Y, W, alpha, iterations)\n",
        "\n",
        "# Print the final parameters and cost history\n",
        "print(\"Final Parameters:\", final_params)\n",
        "print(\"Cost History:\", cost_history)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evAIbd0IjZEx",
        "outputId": "64e6c29a-3be5-4541-9dcf-25bdc3c984c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Parameters: [0.20551667 0.54295081 0.10388027]\n",
            "Cost History: [np.float64(0.21422394189320307), np.float64(0.21269761199879803), np.float64(0.21119652631361235), np.float64(0.20972025896641117), np.float64(0.2082683912857068), np.float64(0.2068405116780125), np.float64(0.2054362155081552), np.float64(0.2040551049816124), np.float64(0.20269678902883861), np.float64(0.2013608831915474), np.float64(0.2000470095109174), np.float64(0.19875479641768753), np.float64(0.19748387862411218), np.float64(0.19623389701774197), np.float64(0.1950044985570019), np.float64(0.1937953361685344), np.float64(0.19260606864627902), np.float64(0.19143636055225827), np.float64(0.1902858821190413), np.float64(0.18915430915385684), np.float64(0.18804132294432793), np.float64(0.18694661016580033), np.float64(0.18586986279023826), np.float64(0.18481077799666035), np.float64(0.18376905808309085), np.float64(0.182744410379999), np.float64(0.18173654716520246), np.float64(0.18074518558021005), np.float64(0.17977004754797837), np.float64(0.17881085969206015), np.float64(0.17786735325711905), np.float64(0.17693926403078863), np.float64(0.17602633226685335), np.float64(0.17512830260972773), np.float64(0.1742449240202133), np.float64(0.17337594970251016), np.float64(0.17252113703246413), np.float64(0.17168024748702557), np.float64(0.17085304657490266), np.float64(0.17003930376838602), np.float64(0.1692387924363272), np.float64(0.16845128977824977), np.float64(0.16767657675957526), np.float64(0.1669144380479437), np.float64(0.16616466195061164), np.float64(0.1654270403529085), np.float64(0.16470136865773363), np.float64(0.16398744572607635), np.float64(0.16328507381854226), np.float64(0.16259405853786774), np.float64(0.16191420877240706), np.float64(0.16124533664057478), np.float64(0.16058725743622781), np.float64(0.15993978957497107), np.float64(0.159302754541371), np.float64(0.15867597683706178), np.float64(0.15805928392972918), np.float64(0.1574525062029569), np.float64(0.1568554769069211), np.float64(0.15626803210991877), np.float64(0.1556900106507156), np.float64(0.1551212540916998), np.float64(0.1545616066728281), np.float64(0.15401091526635027), np.float64(0.15346902933229978), np.float64(0.15293580087473624), np.float64(0.15241108439872897), np.float64(0.15189473686806687), np.float64(0.1513866176636841), np.float64(0.15088658854278855), np.float64(0.15039451359868147), np.float64(0.14991025922125642), np.float64(0.14943369405816653), np.float64(0.14896468897664825), np.float64(0.1485031170259904), np.float64(0.14804885340063823), np.float64(0.1476017754039214), np.float64(0.14716176241239498), np.float64(0.1467286958407838), np.float64(0.14630245910751918), np.float64(0.14588293760085932), np.float64(0.14547001864558134), np.float64(0.14506359147023742), np.float64(0.14466354717496466), np.float64(0.1442697786998386), np.float64(0.14388218079376278), np.float64(0.14350064998388365), np.float64(0.14312508454552297), np.float64(0.1427553844726187), np.float64(0.14239145144866572), np.float64(0.1420331888181477), np.float64(0.14168050155845247), np.float64(0.141333296252262), np.float64(0.14099148106040924), np.float64(0.1406549656951943), np.float64(0.14032366139415145), np.float64(0.1399974808942598), np.float64(0.13967633840659047), np.float64(0.13936014959138185), np.float64(0.13904883153353687), np.float64(0.1387423027185343), np.float64(0.1384404830087475), np.float64(0.1381432936201637), np.float64(0.1378506570994967), np.float64(0.137562497301687), np.float64(0.1372787393677819), np.float64(0.13699930970319016), np.float64(0.1367241359563039), np.float64(0.13645314699748246), np.float64(0.13618627289839122), np.float64(0.13592344491169), np.float64(0.13566459545106507), np.float64(0.13540965807159863), np.float64(0.1351585674504701), np.float64(0.13491125936798423), np.float64(0.13466767068891938), np.float64(0.13442773934419194), np.float64(0.13419140431283003), np.float64(0.13395860560425255), np.float64(0.1337292842408479), np.float64(0.13350338224084696), np.float64(0.1332808426014858), np.float64(0.1330616092824533), np.float64(0.13284562718961865), np.float64(0.13263284215903354), np.float64(0.13242320094120558), np.float64(0.13221665118563727), np.float64(0.13201314142562617), np.float64(0.1318126210633228), np.float64(0.13161504035504046), np.float64(0.13142035039681396), np.float64(0.13122850311020237), np.float64(0.13103945122833172), np.float64(0.13085314828217418), np.float64(0.1306695485870585), np.float64(0.13048860722940933), np.float64(0.13031028005371023), np.float64(0.13013452364968747), np.float64(0.1299612953397103), np.float64(0.12979055316640456), np.float64(0.12962225588047546), np.float64(0.12945636292873622), np.float64(0.1292928344423398), np.float64(0.12913163122520863), np.float64(0.12897271474266087), np.float64(0.1288160471102284), np.float64(0.12866159108266434), np.float64(0.12850931004313595), np.float64(0.12835916799260091), np.float64(0.1282111295393627), np.float64(0.12806515988880282), np.float64(0.1279212248332866), np.float64(0.12777929074223984), np.float64(0.1276393245523929), np.float64(0.12750129375819014), np.float64(0.12736516640236153), np.float64(0.1272309110666531), np.float64(0.1270984968627151), np.float64(0.12696789342314324), np.float64(0.1268390708926723), np.float64(0.12671199991951793), np.float64(0.12658665164686533), np.float64(0.12646299770450173), np.float64(0.1263410102005903), np.float64(0.12622066171358307), np.float64(0.12610192528427094), np.float64(0.12598477440796768), np.float64(0.1258691830268266), np.float64(0.1257551255222865), np.float64(0.12564257670764595), np.float64(0.125531511820763), np.float64(0.12542190651687796), np.float64(0.12531373686155803), np.float64(0.12520697932376107), np.float64(0.12510161076901619), np.float64(0.12499760845272072), np.float64(0.12489495001354944), np.float64(0.12479361346697586), np.float64(0.12469357719890274), np.float64(0.12459481995940072), np.float64(0.1244973208565524), np.float64(0.12440105935040062), np.float64(0.12430601524699941), np.float64(0.12421216869256503), np.float64(0.12411950016772619), np.float64(0.1240279904818715), np.float64(0.1239376207675925), np.float64(0.12384837247522043), np.float64(0.12376022736745575), np.float64(0.1236731675140882), np.float64(0.12358717528680627), np.float64(0.12350223335409431), np.float64(0.12341832467621602), np.float64(0.1233354325002828), np.float64(0.12325354035540556), np.float64(0.12317263204792811), np.float64(0.1230926916567416), np.float64(0.1230137035286781), np.float64(0.12293565227398189), np.float64(0.1228585227618574), np.float64(0.12278230011609251), np.float64(0.1227069697107559), np.float64(0.12263251716596704), np.float64(0.12255892834373741), np.float64(0.12248618934388286), np.float64(0.12241428650000437), np.float64(0.12234320637553682), np.float64(0.12227293575986505), np.float64(0.12220346166450505), np.float64(0.12213477131935015), np.float64(0.12206685216898036), np.float64(0.12199969186903432), np.float64(0.12193327828264255), np.float64(0.1218675994769208), np.float64(0.1218026437195232), np.float64(0.12173839947525318), np.float64(0.12167485540273176), np.float64(0.12161200035112266), np.float64(0.12154982335691224), np.float64(0.12148831364074386), np.float64(0.12142746060430652), np.float64(0.12136725382727505), np.float64(0.12130768306430283), np.float64(0.12124873824206513), np.float64(0.1211904094563522), np.float64(0.12113268696921198), np.float64(0.12107556120614067), np.float64(0.12101902275332109), np.float64(0.1209630623549076), np.float64(0.12090767091035699), np.float64(0.12085283947180456), np.float64(0.12079855924148432), np.float64(0.12074482156919317), np.float64(0.12069161794979763), np.float64(0.12063894002078301), np.float64(0.12058677955984372), np.float64(0.1205351284825145), np.float64(0.12048397883984187), np.float64(0.12043332281609458), np.float64(0.120383152726513), np.float64(0.12033346101509652), np.float64(0.12028424025242851), np.float64(0.12023548313353766), np.float64(0.12018718247579592), np.float64(0.12013933121685176), np.float64(0.1200919224125983), np.float64(0.12004494923517621), np.float64(0.11999840497100957), np.float64(0.119952283018876), np.float64(0.11990657688800842), np.float64(0.11986128019622966), np.float64(0.11981638666811811), np.float64(0.11977189013320469), np.float64(0.11972778452420013), np.float64(0.11968406387525221), np.float64(0.11964072232023279), np.float64(0.11959775409105328), np.float64(0.11955515351600907), np.float64(0.11951291501815157), np.float64(0.11947103311368816), np.float64(0.1194295024104088), np.float64(0.11938831760613948), np.float64(0.11934747348722193), np.float64(0.11930696492701856), np.float64(0.11926678688444337), np.float64(0.11922693440251697), np.float64(0.11918740260694659), np.float64(0.11914818670472992), np.float64(0.11910928198278221), np.float64(0.11907068380658745), np.float64(0.11903238761887125), np.float64(0.118994388938297), np.float64(0.1189566833581838), np.float64(0.11891926654524591), np.float64(0.1188821342383538), np.float64(0.11884528224731584), np.float64(0.11880870645168098), np.float64(0.11877240279956115), np.float64(0.11873636730647419), np.float64(0.11870059605420569), np.float64(0.11866508518969066), np.float64(0.11862983092391371), np.float64(0.11859482953082796), np.float64(0.1185600773462924), np.float64(0.11852557076702676), np.float64(0.11849130624958452), np.float64(0.11845728030934308), np.float64(0.11842348951951101), np.float64(0.11838993051015209), np.float64(0.11835659996722583), np.float64(0.11832349463164424), np.float64(0.11829061129834463), np.float64(0.11825794681537785), np.float64(0.11822549808301219), np.float64(0.11819326205285234), np.float64(0.11816123572697323), np.float64(0.11812941615706839), np.float64(0.11809780044361308), np.float64(0.1180663857350411), np.float64(0.1180351692269359), np.float64(0.1180041481612351), np.float64(0.11797331982544865), np.float64(0.1179426815518901), np.float64(0.11791223071692081), np.float64(0.11788196474020714), np.float64(0.11785188108399001), np.float64(0.11782197725236689), np.float64(0.11779225079058586), np.float64(0.11776269928435178), np.float64(0.1177333203591439), np.float64(0.11770411167954535), np.float64(0.11767507094858358), np.float64(0.11764619590708235), np.float64(0.11761748433302431), np.float64(0.1175889340409247), np.float64(0.11756054288121537), np.float64(0.11753230873963923), np.float64(0.11750422953665522), np.float64(0.11747630322685282), np.float64(0.11744852779837713), np.float64(0.11742090127236303), np.float64(0.11739342170237942), np.float64(0.11736608717388221), np.float64(0.11733889580367743), np.float64(0.11731184573939275), np.float64(0.11728493515895806), np.float64(0.11725816227009504), np.float64(0.11723152530981512), np.float64(0.11720502254392601), np.float64(0.11717865226654672), np.float64(0.11715241279963057), np.float64(0.11712630249249628), np.float64(0.11710031972136714), np.float64(0.11707446288891776), np.float64(0.11704873042382875), np.float64(0.11702312078034872), np.float64(0.1169976324378636), np.float64(0.11697226390047354), np.float64(0.11694701369657676), np.float64(0.11692188037846035), np.float64(0.11689686252189838), np.float64(0.11687195872575613), np.float64(0.11684716761160184), np.float64(0.11682248782332426), np.float64(0.11679791802675717), np.float64(0.11677345690931004), np.float64(0.1167491031796049), np.float64(0.11672485556711944), np.float64(0.11670071282183622), np.float64(0.11667667371389756), np.float64(0.11665273703326642), np.float64(0.1166289015893931), np.float64(0.11660516621088735), np.float64(0.11658152974519619), np.float64(0.11655799105828717), np.float64(0.1165345490343369), np.float64(0.11651120257542477), np.float64(0.1164879506012322), np.float64(0.11646479204874653), np.float64(0.11644172587197021), np.float64(0.11641875104163471), np.float64(0.11639586654491951), np.float64(0.11637307138517558), np.float64(0.11635036458165368), np.float64(0.11632774516923723), np.float64(0.1163052121981797), np.float64(0.11628276473384633), np.float64(0.11626040185646033), np.float64(0.11623812266085355), np.float64(0.11621592625622079), np.float64(0.11619381176587884), np.float64(0.11617177832702949), np.float64(0.11614982509052611), np.float64(0.11612795122064483), np.float64(0.11610615589485901), np.float64(0.11608443830361793), np.float64(0.11606279765012895), np.float64(0.11604123315014338), np.float64(0.1160197440317464), np.float64(0.11599832953514966), np.float64(0.11597698891248827), np.float64(0.11595572142762073), np.float64(0.11593452635593231), np.float64(0.11591340298414188), np.float64(0.11589235061011186), np.float64(0.11587136854266147), np.float64(0.11585045610138303), np.float64(0.11582961261646167), np.float64(0.11580883742849751), np.float64(0.11578812988833147), np.float64(0.11576748935687362), np.float64(0.11574691520493456), np.float64(0.11572640681305965), np.float64(0.11570596357136612), np.float64(0.11568558487938266), np.float64(0.11566527014589201), np.float64(0.1156450187887761), np.float64(0.1156248302348638), np.float64(0.11560470391978125), np.float64(0.11558463928780467), np.float64(0.11556463579171586), np.float64(0.1155446928926596), np.float64(0.11552481006000434), np.float64(0.11550498677120447), np.float64(0.1154852225116651), np.float64(0.11546551677460948), np.float64(0.11544586906094814), np.float64(0.11542627887915066), np.float64(0.1154067457451194), np.float64(0.11538726918206531), np.float64(0.11536784872038601), np.float64(0.1153484838975459), np.float64(0.11532917425795812), np.float64(0.11530991935286868), np.float64(0.11529071874024245), np.float64(0.11527157198465128), np.float64(0.11525247865716359), np.float64(0.11523343833523622), np.float64(0.11521445060260802), np.float64(0.11519551504919504), np.float64(0.11517663127098775), np.float64(0.11515779886994981), np.float64(0.11513901745391866), np.float64(0.11512028663650763), np.float64(0.11510160603701002), np.float64(0.11508297528030437), np.float64(0.11506439399676176), np.float64(0.11504586182215429), np.float64(0.11502737839756537), np.float64(0.11500894336930152), np.float64(0.11499055638880534), np.float64(0.11497221711257051), np.float64(0.11495392520205754), np.float64(0.11493568032361162), np.float64(0.11491748214838132), np.float64(0.11489933035223902), np.float64(0.11488122461570247), np.float64(0.1148631646238577), np.float64(0.11484515006628346), np.float64(0.11482718063697657), np.float64(0.11480925603427874), np.float64(0.11479137596080467), np.float64(0.11477354012337122), np.float64(0.11475574823292801), np.float64(0.11473800000448871), np.float64(0.11472029515706408), np.float64(0.11470263341359578), np.float64(0.11468501450089137), np.float64(0.11466743814956036), np.float64(0.11464990409395162), np.float64(0.11463241207209125), np.float64(0.11461496182562222), np.float64(0.11459755309974451), np.float64(0.11458018564315647), np.float64(0.11456285920799708), np.float64(0.1145455735497893), np.float64(0.11452832842738424), np.float64(0.11451112360290638), np.float64(0.11449395884169972), np.float64(0.1144768339122746), np.float64(0.1144597485862559), np.float64(0.11444270263833144), np.float64(0.11442569584620206), np.float64(0.11440872799053178), np.float64(0.1143917988548992), np.float64(0.11437490822574979), np.float64(0.11435805589234868), np.float64(0.11434124164673456), np.float64(0.11432446528367399), np.float64(0.11430772660061696), np.float64(0.11429102539765273), np.float64(0.11427436147746668), np.float64(0.1142577346452979), np.float64(0.11424114470889732), np.float64(0.11422459147848672), np.float64(0.11420807476671828), np.float64(0.114191594388635), np.float64(0.11417515016163159), np.float64(0.11415874190541607), np.float64(0.11414236944197213), np.float64(0.11412603259552213), np.float64(0.11410973119249035), np.float64(0.11409346506146749), np.float64(0.11407723403317514), np.float64(0.11406103794043132), np.float64(0.11404487661811635), np.float64(0.11402874990313937), np.float64(0.11401265763440527), np.float64(0.11399659965278268), np.float64(0.1139805758010717), np.float64(0.11396458592397292), np.float64(0.11394862986805648), np.float64(0.11393270748173198), np.float64(0.11391681861521857), np.float64(0.11390096312051583), np.float64(0.113885140851375), np.float64(0.1138693516632706), np.float64(0.1138535954133729), np.float64(0.11383787196052028), np.float64(0.11382218116519266), np.float64(0.11380652288948488), np.float64(0.11379089699708082), np.float64(0.11377530335322797), np.float64(0.11375974182471207), np.float64(0.11374421227983271), np.float64(0.11372871458837888), np.float64(0.11371324862160526), np.float64(0.1136978142522086), np.float64(0.11368241135430478), np.float64(0.11366703980340613), np.float64(0.11365169947639917), np.float64(0.11363639025152247), np.float64(0.1136211120083455), np.float64(0.11360586462774705), np.float64(0.11359064799189461), np.float64(0.11357546198422387), np.float64(0.11356030648941852), np.float64(0.11354518139339057), np.float64(0.1135300865832607), np.float64(0.11351502194733933), np.float64(0.11349998737510764), np.float64(0.11348498275719907), np.float64(0.11347000798538132), np.float64(0.11345506295253824), np.float64(0.11344014755265239), np.float64(0.11342526168078763), np.float64(0.11341040523307219), np.float64(0.11339557810668197), np.float64(0.11338078019982412), np.float64(0.11336601141172072), np.float64(0.11335127164259313), np.float64(0.11333656079364621), np.float64(0.11332187876705296), np.float64(0.1133072254659395), np.float64(0.11329260079437013), np.float64(0.11327800465733281), np.float64(0.11326343696072481), np.float64(0.11324889761133845), np.float64(0.11323438651684738), np.float64(0.11321990358579283), np.float64(0.11320544872757028), np.float64(0.11319102185241622), np.float64(0.11317662287139517), np.float64(0.11316225169638684), np.float64(0.11314790824007384), np.float64(0.11313359241592902), np.float64(0.11311930413820367), np.float64(0.11310504332191526), np.float64(0.11309080988283601), np.float64(0.11307660373748117), np.float64(0.11306242480309786), np.float64(0.11304827299765373), np.float64(0.11303414823982605), np.float64(0.1130200504489911), np.float64(0.11300597954521326), np.float64(0.11299193544923496), np.float64(0.11297791808246613), np.float64(0.11296392736697435), np.float64(0.1129499632254747), np.float64(0.1129360255813202), np.float64(0.11292211435849227), np.float64(0.11290822948159113), np.float64(0.11289437087582664), np.float64(0.11288053846700934), np.float64(0.11286673218154103), np.float64(0.11285295194640663), np.float64(0.11283919768916484), np.float64(0.11282546933794016), np.float64(0.11281176682141433), np.float64(0.11279809006881791), np.float64(0.11278443900992242), np.float64(0.1127708135750323), np.float64(0.11275721369497715), np.float64(0.11274363930110379), np.float64(0.11273009032526908), np.float64(0.11271656669983206), np.float64(0.11270306835764694), np.float64(0.11268959523205578), np.float64(0.11267614725688133), np.float64(0.11266272436642016), np.float64(0.11264932649543599), np.float64(0.11263595357915249), np.float64(0.1126226055532471), np.float64(0.1126092823538443), np.float64(0.11259598391750933), np.float64(0.11258271018124177), np.float64(0.11256946108246933), np.float64(0.11255623655904196), np.float64(0.11254303654922566), np.float64(0.1125298609916966), np.float64(0.11251670982553554), np.float64(0.11250358299022185), np.float64(0.11249048042562802), np.float64(0.11247740207201426), np.float64(0.11246434787002298), np.float64(0.11245131776067334), np.float64(0.11243831168535622), np.float64(0.11242532958582897), np.float64(0.11241237140421047), np.float64(0.11239943708297574), np.float64(0.11238652656495156), np.float64(0.1123736397933113), np.float64(0.11236077671157034), np.float64(0.11234793726358126), np.float64(0.11233512139352944), np.float64(0.11232232904592838), np.float64(0.11230956016561536), np.float64(0.11229681469774694), np.float64(0.11228409258779495), np.float64(0.11227139378154193), np.float64(0.1122587182250772), np.float64(0.11224606586479269), np.float64(0.11223343664737893), np.float64(0.11222083051982112), np.float64(0.11220824742939516), np.float64(0.112195687323664), np.float64(0.11218315015047374), np.float64(0.11217063585794985), np.float64(0.11215814439449381), np.float64(0.11214567570877923), np.float64(0.11213322974974854), np.float64(0.11212080646660946), np.float64(0.11210840580883147), np.float64(0.1120960277261427), np.float64(0.11208367216852645), np.float64(0.112071339086218), np.float64(0.11205902842970139), np.float64(0.11204674014970638), np.float64(0.11203447419720532), np.float64(0.11202223052340995), np.float64(0.1120100090797687), np.float64(0.11199780981796353), np.float64(0.11198563268990717), np.float64(0.11197347764774021), np.float64(0.11196134464382819), np.float64(0.11194923363075919), np.float64(0.11193714456134077), np.float64(0.11192507738859743), np.float64(0.11191303206576808), np.float64(0.11190100854630332), np.float64(0.111889006783863), np.float64(0.1118770267323137), np.float64(0.11186506834572627), np.float64(0.11185313157837332), np.float64(0.11184121638472708), np.float64(0.11182932271945678), np.float64(0.11181745053742657), np.float64(0.11180559979369324), np.float64(0.11179377044350368), np.float64(0.11178196244229323), np.float64(0.11177017574568301), np.float64(0.11175841030947817), np.float64(0.11174666608966556), np.float64(0.11173494304241176), np.float64(0.11172324112406111), np.float64(0.11171156029113367), np.float64(0.11169990050032326), np.float64(0.11168826170849552), np.float64(0.11167664387268605), np.float64(0.1116650469500986), np.float64(0.111653470898103), np.float64(0.11164191567423376), np.float64(0.1116303812361879), np.float64(0.11161886754182328), np.float64(0.11160737454915723), np.float64(0.11159590221636424), np.float64(0.1115844505017749), np.float64(0.11157301936387391), np.float64(0.1115616087612985), np.float64(0.11155021865283696), np.float64(0.11153884899742691), np.float64(0.11152749975415395), np.float64(0.11151617088225006), np.float64(0.11150486234109198), np.float64(0.11149357409019998), np.float64(0.11148230608923626), np.float64(0.11147105829800355), np.float64(0.1114598306764437), np.float64(0.1114486231846364), np.float64(0.11143743578279766), np.float64(0.11142626843127865), np.float64(0.11141512109056421), np.float64(0.11140399372127173), np.float64(0.11139288628414973), np.float64(0.11138179874007667), np.float64(0.11137073105005976), np.float64(0.11135968317523372), np.float64(0.11134865507685937), np.float64(0.11133764671632285), np.float64(0.11132665805513417), np.float64(0.11131568905492617), np.float64(0.11130473967745336), np.float64(0.11129380988459076), np.float64(0.111282899638333), np.float64(0.11127200890079304), np.float64(0.11126113763420127), np.float64(0.11125028580090422), np.float64(0.11123945336336394), np.float64(0.11122864028415665), np.float64(0.11121784652597176), np.float64(0.11120707205161115), np.float64(0.11119631682398799), np.float64(0.11118558080612582), np.float64(0.11117486396115757), np.float64(0.11116416625232485), np.float64(0.11115348764297682), np.float64(0.11114282809656935), np.float64(0.11113218757666422), np.float64(0.11112156604692819), np.float64(0.11111096347113213), np.float64(0.11110037981315021), np.float64(0.11108981503695904), np.float64(0.11107926910663697), np.float64(0.11106874198636306), np.float64(0.11105823364041653), np.float64(0.11104774403317585), np.float64(0.11103727312911793), np.float64(0.1110268208928175), np.float64(0.11101638728894626), np.float64(0.11100597228227219), np.float64(0.11099557583765872), np.float64(0.11098519792006424), np.float64(0.11097483849454122), np.float64(0.11096449752623545), np.float64(0.11095417498038561), np.float64(0.1109438708223224), np.float64(0.11093358501746785), np.float64(0.11092331753133491), np.float64(0.11091306832952645), np.float64(0.1109028373777349), np.float64(0.11089262464174166), np.float64(0.11088243008741612), np.float64(0.11087225368071549), np.float64(0.11086209538768385), np.float64(0.11085195517445186), np.float64(0.11084183300723595), np.float64(0.11083172885233784), np.float64(0.11082164267614393), np.float64(0.11081157444512484), np.float64(0.11080152412583467), np.float64(0.11079149168491069), np.float64(0.11078147708907268), np.float64(0.11077148030512236), np.float64(0.11076150129994294), np.float64(0.11075154004049863), np.float64(0.1107415964938341), np.float64(0.11073167062707388), np.float64(0.11072176240742206), np.float64(0.11071187180216167), np.float64(0.11070199877865426), np.float64(0.11069214330433941), np.float64(0.1106823053467342), np.float64(0.11067248487343294), np.float64(0.11066268185210644), np.float64(0.11065289625050183), np.float64(0.11064312803644193), np.float64(0.11063337717782494), np.float64(0.11062364364262389), np.float64(0.11061392739888637), np.float64(0.11060422841473402), np.float64(0.11059454665836199), np.float64(0.11058488209803878), np.float64(0.1105752347021058), np.float64(0.11056560443897669), np.float64(0.11055599127713732), np.float64(0.11054639518514517), np.float64(0.11053681613162895), np.float64(0.11052725408528831), np.float64(0.11051770901489341), np.float64(0.1105081808892847), np.float64(0.11049866967737228), np.float64(0.11048917534813572), np.float64(0.11047969787062378), np.float64(0.11047023721395391), np.float64(0.11046079334731192), np.float64(0.11045136623995178), np.float64(0.11044195586119507), np.float64(0.11043256218043093), np.float64(0.11042318516711545), np.float64(0.11041382479077143), np.float64(0.11040448102098831), np.float64(0.11039515382742138), np.float64(0.11038584317979189), np.float64(0.11037654904788659), np.float64(0.11036727140155737), np.float64(0.11035801021072106), np.float64(0.11034876544535901), np.float64(0.11033953707551701), np.float64(0.11033032507130473), np.float64(0.11032112940289565), np.float64(0.11031195004052669), np.float64(0.11030278695449791), np.float64(0.11029364011517226), np.float64(0.11028450949297533), np.float64(0.1102753950583951), np.float64(0.1102662967819816), np.float64(0.11025721463434669), np.float64(0.11024814858616373), np.float64(0.11023909860816751), np.float64(0.11023006467115375), np.float64(0.11022104674597906), np.float64(0.11021204480356055), np.float64(0.11020305881487565), np.float64(0.11019408875096186), np.float64(0.11018513458291657), np.float64(0.11017619628189654), np.float64(0.11016727381911812), np.float64(0.11015836716585668), np.float64(0.11014947629344649), np.float64(0.11014060117328041), np.float64(0.11013174177680991), np.float64(0.1101228980755445), np.float64(0.1101140700410517), np.float64(0.11010525764495695), np.float64(0.11009646085894312), np.float64(0.11008767965475044), np.float64(0.11007891400417633), np.float64(0.11007016387907508), np.float64(0.11006142925135773), np.float64(0.11005271009299186), np.float64(0.1100440063760013), np.float64(0.11003531807246607), np.float64(0.11002664515452207), np.float64(0.11001798759436081), np.float64(0.11000934536422959), np.float64(0.11000071843643078), np.float64(0.10999210678332201), np.float64(0.10998351037731588), np.float64(0.10997492919087969), np.float64(0.10996636319653533), np.float64(0.1099578123668592), np.float64(0.10994927667448169), np.float64(0.10994075609208752), np.float64(0.109932250592415), np.float64(0.10992376014825629), np.float64(0.109915284732457), np.float64(0.10990682431791607), np.float64(0.10989837887758559), np.float64(0.10988994838447068), np.float64(0.10988153281162925), np.float64(0.10987313213217194), np.float64(0.10986474631926178), np.float64(0.10985637534611416), np.float64(0.10984801918599674), np.float64(0.10983967781222898), np.float64(0.10983135119818242), np.float64(0.1098230393172801), np.float64(0.10981474214299666), np.float64(0.10980645964885818), np.float64(0.10979819180844183), np.float64(0.10978993859537596), np.float64(0.10978169998333978), np.float64(0.10977347594606332), np.float64(0.10976526645732715), np.float64(0.10975707149096237), np.float64(0.10974889102085035), np.float64(0.10974072502092272), np.float64(0.10973257346516105), np.float64(0.10972443632759694), np.float64(0.10971631358231154), np.float64(0.1097082052034358), np.float64(0.10970011116515), np.float64(0.10969203144168382), np.float64(0.1096839660073162), np.float64(0.10967591483637494), np.float64(0.10966787790323693), np.float64(0.1096598551823278), np.float64(0.10965184664812183), np.float64(0.10964385227514184), np.float64(0.10963587203795898), np.float64(0.10962790591119274), np.float64(0.1096199538695107), np.float64(0.10961201588762844), np.float64(0.1096040919403094), np.float64(0.10959618200236483), np.float64(0.10958828604865356), np.float64(0.10958040405408187), np.float64(0.10957253599360357), np.float64(0.10956468184221951), np.float64(0.10955684157497782), np.float64(0.1095490151669736), np.float64(0.10954120259334879), np.float64(0.10953340382929216), np.float64(0.10952561885003916), np.float64(0.10951784763087159), np.float64(0.10951009014711792), np.float64(0.10950234637415272), np.float64(0.1094946162873968), np.float64(0.10948689986231708), np.float64(0.10947919707442641), np.float64(0.10947150789928335), np.float64(0.10946383231249242), np.float64(0.10945617028970356), np.float64(0.10944852180661231), np.float64(0.1094408868389595), np.float64(0.10943326536253146), np.float64(0.10942565735315939), np.float64(0.1094180627867198), np.float64(0.10941048163913401), np.float64(0.10940291388636826), np.float64(0.10939535950443353), np.float64(0.10938781846938535), np.float64(0.10938029075732388), np.float64(0.10937277634439366), np.float64(0.10936527520678356), np.float64(0.10935778732072658), np.float64(0.1093503126625), np.float64(0.10934285120842502), np.float64(0.10933540293486668), np.float64(0.10932796781823395), np.float64(0.10932054583497941), np.float64(0.10931313696159928), np.float64(0.10930574117463332), np.float64(0.10929835845066466), np.float64(0.1092909887663197), np.float64(0.10928363209826811), np.float64(0.10927628842322266), np.float64(0.1092689577179391), np.float64(0.10926163995921614), np.float64(0.10925433512389526), np.float64(0.10924704318886078), np.float64(0.10923976413103943), np.float64(0.10923249792740071), np.float64(0.10922524455495645), np.float64(0.10921800399076081), np.float64(0.10921077621191025), np.float64(0.10920356119554335), np.float64(0.10919635891884077), np.float64(0.10918916935902521), np.float64(0.10918199249336118), np.float64(0.10917482829915497), np.float64(0.1091676767537546), np.float64(0.10916053783454975), np.float64(0.10915341151897164), np.float64(0.1091462977844927), np.float64(0.10913919660862695), np.float64(0.1091321079689296), np.float64(0.10912503184299699), np.float64(0.10911796820846656), np.float64(0.10911091704301669), np.float64(0.10910387832436674), np.float64(0.1090968520302769), np.float64(0.10908983813854801), np.float64(0.10908283662702158), np.float64(0.10907584747357969), np.float64(0.10906887065614493), np.float64(0.1090619061526802), np.float64(0.10905495394118875), np.float64(0.10904801399971409), np.float64(0.10904108630633977), np.float64(0.10903417083918945), np.float64(0.10902726757642679), np.float64(0.10902037649625529), np.float64(0.10901349757691824), np.float64(0.10900663079669869), np.float64(0.10899977613391931), np.float64(0.10899293356694234), np.float64(0.10898610307416945), np.float64(0.10897928463404175), np.float64(0.10897247822503968), np.float64(0.1089656838256829), np.float64(0.10895890141453021), np.float64(0.10895213097017944), np.float64(0.10894537247126751), np.float64(0.10893862589647023), np.float64(0.1089318912245022), np.float64(0.10892516843411676), np.float64(0.10891845750410602), np.float64(0.1089117584133007), np.float64(0.10890507114056985), np.float64(0.10889839566482128), np.float64(0.10889173196500088), np.float64(0.10888508002009299), np.float64(0.10887843980912004), np.float64(0.1088718113111428), np.float64(0.10886519450525993), np.float64(0.10885858937060817), np.float64(0.10885199588636209), np.float64(0.10884541403173416), np.float64(0.10883884378597458), np.float64(0.10883228512837129), np.float64(0.10882573803824976), np.float64(0.10881920249497302), np.float64(0.10881267847794161), np.float64(0.10880616596659343), np.float64(0.10879966494040369), np.float64(0.10879317537888483), np.float64(0.10878669726158649), np.float64(0.10878023056809534), np.float64(0.10877377527803518), np.float64(0.10876733137106667), np.float64(0.1087608988268874), np.float64(0.10875447762523173), np.float64(0.10874806774587079), np.float64(0.10874166916861233), np.float64(0.10873528187330074), np.float64(0.10872890583981683), np.float64(0.10872254104807796), np.float64(0.10871618747803792), np.float64(0.10870984510968663)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To-Do 8: Implementation of RMSE\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def rmse(Y, Y_pred):\n",
        "    \"\"\"\n",
        "    This Function calculates the Root Mean Squares.\n",
        "\n",
        "    Input Arguments:\n",
        "    Y: Array of actual(Target) Dependent Variables.\n",
        "    Y_pred: Array of predicted Dependent Variables.\n",
        "\n",
        "    Output Arguments:\n",
        "    rmse: Root Mean Square Error.\n",
        "    \"\"\"\n",
        "    rmse = np.sqrt(np.mean((Y - Y_pred) ** 2))\n",
        "    return rmse\n",
        "\n",
        "print(\"RMSE function defined successfully\")\n",
        "print(\"\\nFunction: rmse(Y, Y_pred)\")\n",
        "print(\"Calculates Root Mean Square Error\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGEuPjosjiF4",
        "outputId": "70b5f02b-5b8c-4b1d-9874-c9c2154fce64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE function defined successfully\n",
            "\n",
            "Function: rmse(Y, Y_pred)\n",
            "Calculates Root Mean Square Error\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To-Do 9: Implementation of R-Squared Error\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def r2(Y, Y_pred):\n",
        "    \"\"\"\n",
        "    This Function calculates the R Squared Error.\n",
        "\n",
        "    Input Arguments:\n",
        "    Y: Array of actual(Target) Dependent Variables.\n",
        "    Y_pred: Array of predicted Dependent Variables.\n",
        "\n",
        "    Output Arguments:\n",
        "    r2: R Squared Error.\n",
        "    \"\"\"\n",
        "    mean_y = np.mean(Y)\n",
        "    ss_tot = np.sum((Y - mean_y) ** 2)\n",
        "    ss_res = np.sum((Y - Y_pred) ** 2)\n",
        "    r2 = 1 - (ss_res / ss_tot)\n",
        "    return r2\n",
        "\n",
        "print(\"R-Squared function defined successfully\")\n",
        "print(\"\\nFunction: r2(Y, Y_pred)\")\n",
        "print(\"Calculates R-Squared (coefficient of determination)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZJC4Wifjrm5",
        "outputId": "ee36f9a7-f8bb-41ca-bbea-47fadf2e7f9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R-Squared function defined successfully\n",
            "\n",
            "Function: r2(Y, Y_pred)\n",
            "Calculates R-Squared (coefficient of determination)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To-Do 10: Compiling everything together\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def main():\n",
        "    # Step 1: Load the dataset\n",
        "    data = pd.read_csv('student.csv')\n",
        "\n",
        "    # Step 2: Split the data into features (X) and target (Y)\n",
        "    X = data.iloc[:, [0, 1]].values  # Features: Math and Reading marks\n",
        "    Y = data.iloc[:, 2].values  # Target: Writing marks\n",
        "\n",
        "    # Step 3: Split the data into training and test sets (80% train, 20% test)\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Step 4: Initialize weights (W) to zeros, learning rate and number of iterations\n",
        "    W = np.zeros(X_train.shape[1])  # Initialize weights\n",
        "    alpha = 0.00001  # Learning rate\n",
        "    iterations = 1000  # Number of iterations for gradient descent\n",
        "\n",
        "    # Step 5: Perform Gradient Descent\n",
        "    W_optimal, cost_history = gradient_descent(X_train, Y_train, W, alpha, iterations)\n",
        "\n",
        "    # Step 6: Make predictions on the test set\n",
        "    Y_pred = np.dot(X_test, W_optimal)\n",
        "\n",
        "    # Step 7: Evaluate the model using RMSE and R-Squared\n",
        "    model_rmse = rmse(Y_test, Y_pred)\n",
        "    model_r2 = r2(Y_test, Y_pred)\n",
        "\n",
        "    # Step 8: Output the results\n",
        "    print(\"Final Weights:\", W_optimal)\n",
        "    print(\"Cost History (First 10 iterations):\", cost_history[:10])\n",
        "    print(\"RMSE on Test Set:\", model_rmse)\n",
        "    print(\"R-Squared on Test Set:\", model_r2)\n",
        "\n",
        "# Execute the main function\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EbFuViSCj1ws",
        "outputId": "9d5f9032-0615-45ad-83e5-4b95fd25f062"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Weights: [0.34811659 0.64614558]\n",
            "Cost History (First 10 iterations): [np.float64(4026.33114156751), np.float64(3280.573665199384), np.float64(2674.1239989803175), np.float64(2180.9589785701155), np.float64(1779.9166540166468), np.float64(1453.788198601909), np.float64(1188.5794521617188), np.float64(972.910410590327), np.float64(797.5268927198968), np.float64(654.9034294649376)]\n",
            "RMSE on Test Set: 5.2798239764188635\n",
            "R-Squared on Test Set: 0.8886354462786421\n"
          ]
        }
      ]
    }
  ]
}